---
author: bydiao
comments: true
date: 2017-03-08
layout: post
title: 大数据简史
mathjax: true
categories: [Big Data]
tags: [Talk]
---

#### 大数据简史
 
要想探索数据是如何从小变大的，当然不能只从大数据概念被热炒的这几年说起。我们发现，第一次有关数据增长率的记载出现在70多年前。信息爆炸（Information Explosion）的词条在1941年第一次出现。下面是一些有关数据增长，以及大数据概念的里程碑事件。
 
1944 Fremont Rider，Wesleyan大学图书管理员发表了一本名为《 The Scholar and the Future of the Research Library》的著作。书中，他预测了美国大学图书馆将以每16年翻一倍的速度增长。基于这样的增长率，Rider担心耶鲁大学到2040年的时候，书籍将达到大约2亿册，将占用长约6000英里的书架，需要约6000名图书管理员。

（这个预测很有意思，这本书有机会要读一下，一个人在1944年竟然能有这种超前的预测）

1961年，Derek Price发表了《Science Since Babylon》一书，书中他通过统计学术杂志和论文的方式计算科学知识的增长。他发现，新的学术论文以指数趋势增长，而不是线性增长。大约以每15年翻一倍，每半个世纪增长一个数量级的速度递增。Price将此命名为“指数增长法则”（law of exponetial increase），每个科学进步都会以相对固定的产生率，产生新的科学分支。因此产生率严格正比于新发现的产生。

1967年12月，B.A Marron 和 P.A.D.de Maine在ACM通讯上发表了“自动数据压缩”的论文，提出了所谓的信息爆炸，可以通过自动压缩算法，将信息压缩为最小情况。论文提出了一个由三部分组成的压缩机，大大延缓了外部存储的需求，提高了数据在计算机中的传输效率。
（这算是解决了当时那个时代的大数据问题）

1971年，Arthur Miller在隐私攻击《The Assault on Privacy》一书中提到，太多掌握信息的人似乎可以通过一个人档案占用字节的数量来揣摩一个人的隐私。

（这个概念可以算是数据画像的启蒙）

1975年，日本邮电通信部开始主导了一项信息流动普查的工作，跟踪信息在日本各地之间的流通量。普查以单词的数量为统一基础单位计量所有媒体。1975年的普查发现，信息的供给增长速度大大高于信息消费的增长速度。到1978年，那些单向交流的大众媒体的发展几乎已经停滞，双向的个人电信媒体的信息需求却不断飙升。我们的社会正在走向新的台阶，小众市场越来越得到重视，个体的体验越来越得到重视。

1980年4月，I.A. Tjomsland在第四届IEEE大规模存储系统研讨会上发表了一次我们将走向何方《Where Do We Go From Here》的演讲，他提到：那些存储从业人员早就意识到，帕金森第一定律也许很好的解释了我们的产业（帕金森第一定律，demand may expand to match supply， reverse is not true）。我相信大量的数据都将被保留下来，因为用户无法分辨废弃的数据。保留废弃数据的代价，显然低于错过有用数据的代价。
（有人意识到了数据的价值）

1981年，Hungarian 中心统计办公室开始了一项研究计划，统计国家信息工业，包括计算以bit为单位计量信息量。这项研究一直持续到今天。到1993年，Istvan，Hungarian中心首席科学家编纂了一本国家信息产业纪要。

1983年，Ithiel de Sola Pool 在Science发表了 跟踪信息的流动 《tracking the Flow of Information》一文。文中观察了17个主要通讯媒体从1960年到1977年的发展趋势，他得出，被动接受的媒介信息量以每年百分之8.9的速度增长，主动接受的媒介信息量以每年百分之2.9的速率增长。更多的信息量是以广播的形式传播的。但到了1977年底，情况发生了改变，点对点通信量的增长率逐渐超越了广播。Pool等人在1984年发表了另一本书，通信流，一次在日本和美国的普查《Communications Flows: A Census in the United States and Japan》，书中比较了美国和日本产生的信息量规模。
（双向通信带来了巨大的革新，大家意识到反馈数据的价值）

1986年7月，Hal，B.Becker 发表了一篇《Can users really absorb data at today’s rates》论文，他们预测，古腾堡发明的打印机所得到的信息记录密度大约是500字符每立方英寸，是公元前四千年Sumerian人通过黏土记录字符的500倍。到2000年，半导体随机读写内存将达到10的11次方每立方英寸的密度。

（这里从技术的角度预测了人类对数据的存储能力，现在看来，大数据时代，人类存储数据的办法还不成问题）


1990年9月，Peter J.Denning 发表了 《Saving All the Bits》 在美国科学家杂志上。他提到，存储所有的数据已经将我们逼上了绝路，信息增长的速率已经严重压垮了我们的网络系统，存储设备和检索系统，同时超越了人类理解能力的上限。什么样的机器才能帮我们监控数据的流动？他预测，在未来，建造一种可以帮我们识别和预测模式的机器是可能的，这样的机器也许最终能够足够的快来处理所有大规模实时数据流。有了这些机器，我们可以明显的降低必须存储的数据，也可以通过一个巨大的数据仓库和超强的数据处理能力防止我们错过那些实时数据中的大量潜在的有价值的信息。这样的机器也可以分析离线的数据，来发觉其中存在的模式和有用的信息。

(这简直是大数据思维的萌芽，这篇论文一定要看看)

1996年，数码存储设备终于在成本上大大低于纸质存储媒体，详见《The Evolution of Storage Systems》

1997年10月，Michael Cox 和 David Ellsworth 在第八届IEEE视觉论坛上发表了《Application-controlled demand paging for out-of-core visualization》一文，文中提到，可视化带来了计算机系统前所未有的挑战，数据集都非常大，已经超越了内存，磁盘甚至远程磁盘的容量限制。我们叫这个问题为大数据问题。当数据量不能承载在单机存储器上，我们就称之为大数据问题。解决问题的办法，就是需要更多的计算资源支撑。这是ACM电子文献库中第一次提到大数据这个词汇。

1997，Michael Lesk 发表了《How much information is there in the world》世界的信息量有多大一文，lesk的结论是，有几千PB数据，2000年就可以有足够的磁盘或者磁带存储下。也就是说，从当时算起的几年后，人类就可以存储所有的数据，不再会有数据被遗弃。

1998年，John.R Masey， SGI首席科学家发表了大数据，下一波基础设施革新重点

1998年10月，K.G.Coffman 和 Andrew Odlyzko发表了《The Size and Growth Rate of the Internet》一文。他们得出，网络流量以每年百分之百的速度增长，因此，到2002年，数据流量将超过声音网络的流量。Odlyzko随后又发布了《 Minnesota Internet Traffic Studies》米尼苏达网络流量研究，跟踪研究了2002到2009年的网络流量。

1999年8月，Steve Bryson,David Kenwright, Michael Cox等人，发表了《Visually exploring gigabyte data sets in real time》，这是第一篇ACM通信文章用到big data词条。

1999年10月，Bryson等人组织了一个名为《Automation or interaction: what’s best for big data》的研讨会

2000年10月，Peter等人在加州伯克利大学发表了《How Much Information》一文。这是一篇全面研究计算机存储问题的，研究了paper，film，optical， magnetic 四种存储介质。研究发现，1999年，世界产生了大约1.5EB的不重复信息，或大约250MB每人。研究发现，大量的不重复信息被保留在个人计算机上，数码信息的总量不仅最大，其增长率也最高。2003年，全球大约会产生5EB的数据，92%的数据将存储在磁介质上。

2000年12月，Francis X在第八届全球经济论坛上发表论文《’Big Data’ Dynamic Factor Models for Macroeconomic Measurement and Forecasting》，他说道，最近，很多科学研究，无论物理，生物还是社会学，都不得不面对，同时也往往受益于大数据现象。随着大数据规模的不断扩大，这都得益于数据记录与存储技术的发展。

2001年2月，Doug Laney， Meta Group的研究员，发表了一项研究《3D Data Management: Controlling Data Volume, Velocity, and Variety.》十年后，3V被公认为描述大数据的三个维度

2005年，Tim 发表了《What is Web 2.0》一文，他断言，SQL是新的HTML，数据新的Intel inside，数据管理是Web2.0的核心竞争力。因此，对于Web应用，我们与其称之为Software，不如称之为Infoware

2007年，John F 发表了一项白皮书，《The Expanding Digital Universe: A Forecast of Worldwide Information Growth through 2010》这是第一项估计和预测数码数据被制造和转发数量的研究。IDC在2006年预测，全球产生了161EB的数据，并预测到2010年，将达到988EB，或者以每18个月的时间翻倍增长。根据2010和2012发表的类似研究，数码数据的增长率超越了这项预测，在2010年达到了1227EB，并在2012年达到了2837EB

2008年1月，Bret发表了《Estimating the Exaflood》一文，文中他们预测到2015年，美国的网络IP流量将达到1ZB，也就是说到2015年，美国的互联网规模将是2006年的50倍

2008年6月，Cisco发表了《Cisco Visual Networking Index – Forecast and Methodology, 2007–2012》，作为网络预测与网络可视化应用的一部分，预测了IP网络将以每两年翻一倍的速率扩张。到2012年达到0.5ZB。过去五年已经增长了8倍。

2008年9月，《 A special issue of Nature on Big Data》分析了大数据集对当代科学的意义

2008年12月，Randal E发表了《Big-Data Computing: Creating Revolutionary Breakthroughs in Commerce, Science and Society 》一文，正如搜索引擎改变了我们获取信息的方式，大数据计算可以并将改变公司的运营模式，科学研究人员的研究思路，甚至国防情报的获取方式等。大数据计算也许是过去时间计算机领域最大的创新。政府适度的扶持将会加速该领域的发展。

2009年12月，Roger 发表了《How Much Information? 2009 Report on American Consumers.》，研究发现在2008年，美国人消费信息平均每天12小时，产生了3.6ZB，10845T的词汇

2010年2月，Kenneth 发表在经济学人上 Data, data everywhere，无处不在的数据已经越来越被意识到。

2011年2月，Martain Hilbert 在科学上发表了《The World’s Technological Capacity to Store, Communicate, and Compute Information》

2011年5月，James 发表文章《Big data: The next frontier for innovation, competition, and productivity》，他们估计到2009年，美国雇员超过1000人的公司都至少有200TB的数据。加起来，到2010年，公司有7.4EB的新数据，顾客有6.8EB的新数据。

2012年4月，

2012年5月，danah发表《Critical Questions for Big Data》，文中给出了大数据的定义

a cultural, technological, and scholarly phenomenon that rests on the interplay of:  (1) Technology: maximizing computation power and algorithmic accuracy to gather, analyze, link, and compare large data sets. (2) Analysis: drawing on large data sets to identify patterns in order to make economic, social, technical, and legal claims. (3) Mythology: the widespread belief that large data sets offer a higher form of intelligence and knowledge that can generate insights that were previously impossible, with the aura of truth, objectivity, and accuracy.”

来源： [链接](http://www.forbes.com/sites/gilpress/2013/05/09/a-very-short-history-of-big-data/2/#253ebcf01af0)