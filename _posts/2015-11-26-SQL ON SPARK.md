---
author: bydiao
comments: true
date: 2015-11-26 
layout: post
title: SQL on Spark
mathjax: true
categories: [Big Data]
tags: [Spark]
---

##### Sql on spark

Sql 是使用最广泛的数据查询脚本语言，几乎所有大数据工具，都在想方设法的去支持Sql，因为Sql有广大的受众，对Sql支持的好，平台才会有更多的人用，才能保持社区的生命力。

Spark也不例外。一般而言，Sql on spark主要有三大项目，HIVE on Spark，Spark Sql和Shark。他们之间的关系和区别，从HIVE on spark的官方说明上可以看出。 详见以下链接。https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark

摘抄其中说明三者区别的一段：

>1.3 Comparison with Shark and Spark SQL
>There are two related projects in the Spark ecosystem that provide Hive QL support on Spark: Shark and Spark SQL.
>* The Shark project translates query plans generated by Hive into its own representation and executes them over Spark.
>* Spark SQL is a feature in Spark. It uses Hive’s parser as the frontend to provide Hive QL support. Spark application developers can easily express their data processing logic in SQL, as well as the other Spark operators, in their code. Spark SQL supports a different use case than Hive.

>Compared with Shark and Spark SQL, our approach by design supports all existing Hive features, including Hive QL (and any future extension), and Hive’s integration with authorization, monitoring, auditing, and other operational tools.


简单的说，三者都是用了hive作为前端，解析HiveQL语言，但shark用自己的一套表达方式，翻译成spark的任务；spark sql和hive on spark 的区别是，sparksql不需要启动hive，直接兼容hive操作，并且可以结合自身丰富的算子，实现查询后更复杂的操作；hive 则需要基于hive的shell或者hive api，仅将spark作为mapreduce的引擎。

目前Shark项目已经被遗弃，Hive on spark不够灵活。而Spark SQL从spark 1.3.1 有了datgaframe开始，更将具有表现力和生命力。

SparkSQL将成为我们使用的主流，相信也是未来发展的主流。

尽管SparkSQL还不够完善，但对我们来说，已经够用。

##### sparksql 部署和初步测试

sparkSQL是spark的核心模块之一，部署完spark，则原生带有了SparkSQL。但要访问HIVE表，还需要进行一系列配置。

* 版本匹配，目前CDH的版本是5.2.5，基于apache hadoop2.5，因此spark的版本也要相应修改，正好spark几天前刚刚更新到spark-1.3.1。如果不更新版本，则会出现protobuf错误，无法访问HDFS。顺便一起更新了。目前的spark版本是

{% highlight shell %} 
spark-1.3.1-bin-hadoop2.4
{% endhighlight %}

* 将配置好的hive-site.xml 复制到spark/conf下，并在spark-env.sh下将路径加入到classpath

{% highlight shell %} 
#spark-env.sh
export CLASSPATH=$SPARK_HOME/conf
export HIVE_HOME=/home/datascience/hive
{% endhighlight %}

* 要修改hive-site.xml中一些内容，hive中时间的表达都是1s，1000ms，但spark不识别，要去掉所有的s和ms，并全部意ms为单位修改value

{% highlight shell %} 
<value>1s<value>   ===>   <value>1000<value>
{% endhighlight %}

* 要显示的声明将mysql-java-connector.jar 加入到class-path中，以便于访问mysql元数据库。以spark-shell为例，如下：

{% highlight shell %} 
spark-shell --driver-class-path path/to/mysql.jar
{% endhighlight %}

一切就绪，这样就可以在spark-shell中访问hive的表格了。简单写了几行scala代码如下：

{% highlight Scala %} 
val hc = new org.apache.spark.sql.hive.HiveContext(sc)
val tableRDD = hc.sql("select * from aisdata")
tableRDD.cache()
tableRDD.count()
{% endhighlight %}

count了一个19亿行的表，1000多个block，cache后，用了3s出头。同样的操作在oracle服务器下，跑了几分钟才出结果。

未来工作：

1. 继续深入了解SparkSQL和dataframe api
2. 尝试将HDFS的block设置为128MB，对比性能。
3. 尝试设置Spark的executor参数，优化性能。 



