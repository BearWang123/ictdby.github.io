---
author: bydiao
comments: true
date: 2015-10-30
layout: post
title: Spark 编程小结
mathjax: true
categories: [Big Data]
tags: [Spark]
---


1. Clinet 方式提交任务
* 保证client与集群内所有节点通信正常
* client防火墙设置，可以与master和集群内其他节点通信
* 本地Hive，Hadoop等配置文件，要与Master下的保持一致
* 本地的spark-default.conf中，内存不要设置过大，用多少设置多少即可
* spark-submit，会读取本地的spark-default.conf中的参数

2. spark-submit脚本
http://endymecy.gitbooks.io/spark-programming-guide-zh-cn/content/deploying/submitting-applications.html
按这里来即可。


3. 广播变量
{% highlight python %} 
bcst = [1,2,3,4]
bcstvalue = sc.broadcast(bcst)
#这样 在RDD的函数中，就可以引用bcstvalue了
rdd.map(lambda x: x*bcstvalue.value[2])
#其实这种方法，就可以用来实现两重循环
{% endhighlight %}


4. 累加器

{% highlight python %} 
acc = sc.accumulator(0)
#这样，在所有的rdd函数中，可以引用这个累加器进行记数

#一般只在action中使用累加器，workertask不能读取累加器数值
rdd.foreach(lambda x: acc += 1)

#只有driver可以读取累加器数值
print acc.value
{% endhighlight %}